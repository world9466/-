import requests,json,re,os,time
from bs4 import BeautifulSoup
from datetime import datetime
from jsonpath import jsonpath
from fake_useragent import UserAgent
import paramiko
#pip install jieba
#pip install BeautifulSoup4
#pip install jsonpath
#pip install fake-useragent

# Google Trend
# æ•™å­¸ç¶²å€ https://tlyu0419.github.io/2020/02/18/Crawl-GoogleTrends/


# è¨­å®š line æ¨æ’­å‡½å¼
def lineNotifyMessage(token, msg):
    headers = {
        "Authorization": "Bearer " + token, 
        "Content-Type" : "application/x-www-form-urlencoded"
    }

    payload = {'message': msg }
    r = requests.post("https://notify-api.line.me/api/notify", headers = headers, params = payload)
    return r.status_code
token = 'm6uafWsyIziRWXaqYfTKJxNShGYlp3WM3RG9e0hP2OA'


#### ä¸‹è¼‰NLPå­—è©åº«  ####

# å»ºç«‹é€£ç·š
client = paramiko.SSHClient()

client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

client.connect(hostname='10.227.58.87', username='root', password='0266007766')
t = client.get_transport()
sftp=paramiko.SFTPClient.from_transport(t)

try:
    file_list = ['NLP-CN_words.txt','NLP-Event.txt','NLP-Org.txt','NLP-People.txt','NLP-Region.txt','NLP-stopwords.txt']
    for file in file_list:
        print('downloading {}'.format(file))
        sftp.get('/var/www/html/keyword/NLP/{}'.format(file), 'NLP/{}'.format(file))

except Exception as errormsg:
    print('downloading failed...')
    print(errormsg)
    pass

#### ä¸‹è¼‰NLPå­—è©åº«-å®Œæˆ  ####




#### æ“·å–googleé—œéµå­—  ####

time_trend = datetime.now().strftime('%Y%m%d')
time_trend = str(time_trend)

# å˜—è©¦ç·¨ç¢¼æ“·å–è³‡æ–™ï¼Œè‹¥å‡ºç¾éŒ¯èª¤å°±ä¸­æ–·åŸ·è¡Œ

try:
    # ä¿®æ”¹ç¶²å€ç‚ºç›®å‰æ™‚é–“
    trends_url = 'https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&tz=-480&ed={}&geo=TW&ns=15'.format(time_trend)

    res_trends = requests.get(trends_url)

    # æŠŠå¹²æ“¾æ–‡å­— )]}',\n åˆªé™¤ 
    res_trends = re.sub(r'\)\]\}\',\n', '', res_trends.text)

    # è½‰ç¢¼ï¼Œä¸ç„¶ä¸èƒ½çœ‹

    trends = res_trends.encode('utf-8').decode('unicode_escape')


    # æŠŠjsonæª”å†è½‰æˆå­—ä¸²é–‹å§‹åšæ­£å‰‡è™•ç†
    trends_str = str(trends)
    trends_str = trends_str.replace('\"','\'')

    trends_content = re.findall("'query'.*?'snippet':",trends_str)

    trends_title_list = []
    trends_search_list = []
    trends_articles_list = []
    trends_url_list = []
    count_trends = 0

    for content in trends_content:
        count_trends+=1
        # æ´—å‡ºæ¨™é¡Œ
        trends_title = re.findall("'query'.*?,",content)
        trends_title = trends_title[0]
        trends_title = trends_title[9:-2]
        trends_title_list.append(trends_title)

        # æ´—å‡ºæœå°‹é‡
        trends_search = re.findall("formattedTraffic.*?,",content)
        trends_search = trends_search[0]
        trends_search = trends_search[19:-2]
        trends_search_list.append(trends_search)    

        # æ´—å‡ºç¬¬ä¸€å‰‡æ–‡ç« 
        trends_articles = re.findall("'articles.*?timeAgo",content)
        trends_articles = trends_articles[0]
        trends_articles = trends_articles[22:-10]
        trends_articles_list.append(trends_articles)
        
        # æ´—å‡ºæ–‡ç« è¶…é€£çµï¼ŒæŠ“ä¸åˆ°å°±ç”¨googleæœå°‹æ›¿ä»£
        if 'newsUrl' or '\'Url' in content:
            trends_url = re.findall("newsUrl.*?',|'url':.*?',",content)
            trends_url = trends_url[0]
            trends_url = re.findall("http.*?',",content)
            trends_url = trends_url[0]
            trends_url = trends_url[:-2]
            trends_url_list.append(trends_url)
        else:
            trends_url = 'https://www.google.com/search?q='+trends_title
            trends_url_list.append(trends_url)


    print('google access ok')

    print('ç²å–æ¨™é¡Œç­†æ•¸å…±ï¼š {} ç­†'.format(len(trends_title_list)))
    print('ç²å–æœå°‹é‡ç­†æ•¸å…±ï¼š {} ç­†'.format(len(trends_search_list)))
    print('ç²å–å…§æ–‡ç­†æ•¸å…±ï¼š {} ç­†'.format(len(trends_articles_list)))
    print('ç²å–è¶…é€£çµç­†æ•¸å…±ï¼š {} ç­†'.format(len(trends_url_list)))


    ################ ç·¨å¯«ç¶²é å…§å®¹  ################

    rw = open('separate/google.html','w',encoding = 'utf8')
    # r è®€å–
    # w å¯«å…¥(åˆªé™¤åŸæœ¬å…§å®¹)
    # a è¿½åŠ å¯«å…¥


    # å¯«å…¥google trends
    title_trends = '<h2 style="text-align:center;background: #f00; color: #fff; margin: 0 4px; border-radius: 4px;">Googleæœå°‹è¶¨å‹¢</h2>'
    rw.write(title_trends)

    # å¯«å…¥è³‡æ–™æ“·å–æ™‚é–“
    time_now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    time_news = '<div><h3 style="text-align:center;color:gray">è³‡æ–™æ“·å–æ™‚é–“{}</h2></div>'.format(time_now)
    rw.write(time_news)

    for num in range(10):
        begin = '<item>'
        trends_title = (
            '<div style="text-align:left;font-size:20px;">'+
            '<span style="margin-left:20px;font-family:Lucida Console;color:#008000;">({})</span>'.format(num+1)+
            '<span style="margin-left:50px;margin-bottom:5px;font-weight:bold;">{}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>'.format(trends_title_list[num])+
            '<span>ğŸ”¥&nbsp;{}ç­†æœå°‹</span></div>'.format(trends_search_list[num])
            )
        news = '<div style="text-align:left;margin-bottom:25px;margin-left:50px;font-size:20px;"><a href="{}" target="_blank">{}</a></div>'.format(trends_url_list[num],trends_articles_list[num])
        end = '</item>'
        rw.write(begin+trends_title+news+end)
        
except Exception as errormsg:
    print('googleè³‡æ–™ç²å–å¤±æ•—')
    today = datetime.now().strftime('%Y-%m-%d %Hæ™‚')
    message = 'googleè³‡æ–™ç²å–æœ‰èª¤ï¼Œç™¼ç”Ÿæ™‚é–“é»ï¼š{}\n'.format(today)
    lineNotifyMessage(token, message+str(errormsg))
    time.sleep(5)
    os._exit(0)

rw.close()


###å¯«å…¥é—œéµå­—è‡³ txt - trends_title_list

'''æŠŠ Google è³‡æ–™æ‹†åˆ†å¾Œ æ”¾å…¥å­—å…¸'''
import jieba
import jieba.posseg as pseg


try:
    for i in range(len(trends_title_list)): 
        text = trends_title_list[i]
        words = pseg.cut(text)
        print(f'text:{text}')
        for word, flag in words:
            #åŸºæœ¬å­—å…¸
            if len(word) > 1:
                with open('NLP/NLP-CN_words.txt', 'a',encoding='utf-8') as f:
                    f.write(word+"\n")

            #å€‹äººå­—å…¸
            if flag == 'nr' or flag == 'nrfg':
                if len(word) > 2:
                    print("äººåï¼š", word)
                    with open('NLP/NLP-People.txt', 'a',encoding='utf-8') as f:
                        f.write(word+"\n")

            elif flag == 'ns' or flag == 'nrt':
                if len(word) >1:
                    print("åœ°åï¼š", word)
                    with open('NLP/NLP-Region.txt', 'a',encoding='utf-8') as f:
                        f.write(word+"\n")

            elif flag == 'nt':
                if len(word) >1:
                    print("çµ„ç¹”ï¼š", word)
                    with open('NLP/NLP-Org.txt', 'a',encoding='utf-8') as f:
                        f.write(word+"\n")
            else:
                if len(word) >1:
                    print("äº‹ä»¶ï¼š", word)
                    with open('NLP/NLP-Event.txt', 'a',encoding='utf-8') as f:
                        f.write(word+"\n")
    f.close()


    ### å‰”é™¤å·²æœ‰å–®å­—
    stopList = ["NLP/NLP_People.txt","NLP/NLP_Region.txt","NLP_Org"]
    stop = [line.strip() for line in open("NLP/NLP-stopwords.txt",encoding="utf-8").readlines()]
    with open('NLP/NLP-Event.txt', 'a',encoding='utf-8') as f:
                        f.write(word+"\n")


    ''' é‡è¤‡å…§å®¹éæ¿¾'''
    StopList = ['NLP/NLP-Region.txt','NLP/NLP-Org.txt','NLP/NLP-People.txt']

    with open('NLP/NLP-Event.txt', 'r', encoding='utf-8') as f1:
        nlp_event = set(f1.read().splitlines())


    for i in range(len(StopList)):
        with open(StopList[i], 'r', encoding='utf-8') as f2:
            nlp_stop = set(f2.read().splitlines())


        overlap = nlp_event.intersection(nlp_stop)
        if overlap:
            print("æœ‰é‡è¤‡çš„å…§å®¹ï¼š", overlap)
            with open('NLP/NLP-Event.txt', 'r+', encoding='utf-8') as f5:
                lines = f5.readlines()
                f5.seek(0)
                for line in lines:
                    if line.strip() not in overlap:
                        f5.write(line)
                f5.truncate()
            print("é‡è¤‡çš„å…§å®¹å·²è‡ªå‹•åˆªé™¤")
        else:
            print("æ²’æœ‰é‡è¤‡çš„å…§å®¹")

    ### åˆªé™¤é‡è¤‡å­—å…ƒ
    def overwrite(txt):  
        with open(txt, 'r',encoding="utf-8") as f:
            lines = f.readlines()
        lines = list(set(lines))
        with open(txt, 'w',encoding="utf-8") as f:
            f.writelines(lines)

    overwrite('NLP/NLP-People.txt')
    overwrite('NLP/NLP-Region.txt')
    overwrite('NLP/NLP-Event.txt')
    overwrite('NLP/NLP-Org.txt')
    overwrite('NLP/NLP-CN_words.txt')
    overwrite('NLP/NLP-stopwords.txt')    
    
except:
    print('å¯«å…¥é—œéµå­—å¤±æ•—')
    time.sleep(5)
    os._exit(0)



#### ä¸Šå‚³æ›´æ–°çš„NLPå­—è©åº«  ####

try:
    file_list = ['NLP-CN_words.txt','NLP-Event.txt','NLP-Org.txt','NLP-People.txt','NLP-Region.txt','NLP-stopwords.txt']
    for file in file_list:
        print('uploading {}'.format(file))
        sftp.put('NLP/{}'.format(file) , '/var/www/html/keyword/NLP/{}'.format(file))

except Exception as errormsg:
    print('uploading failed...')
    print(errormsg)
    pass

#### ä¸Šå‚³æ›´æ–°çš„NLPå­—è©åº«-å®Œæˆ  ####