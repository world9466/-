import requests,json,re,os,time
from bs4 import BeautifulSoup
from datetime import datetime
from jsonpath import jsonpath
from fake_useragent import UserAgent
import paramiko
#pip install jieba
#pip install BeautifulSoup4
#pip install jsonpath
#pip install fake-useragent

# Google Trend
# æ•™å­¸ç¶²å€ https://tlyu0419.github.io/2020/02/18/Crawl-GoogleTrends/


# è¨­å®š line æ¨æ’­å‡½å¼
def lineNotifyMessage(token, msg):
    headers = {
        "Authorization": "Bearer " + token, 
        "Content-Type" : "application/x-www-form-urlencoded"
    }

    payload = {'message': msg }
    r = requests.post("https://notify-api.line.me/api/notify", headers = headers, params = payload)
    return r.status_code
token = 'yyfusEhNOEMWmOQrmWDmz4vGGnmy59xI4KpzDRRcCAJ'


#### ä¸‹è¼‰NLPå­—è©åº«  ####

# å»ºç«‹é€£ç·š
client = paramiko.SSHClient()

client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

client.connect(hostname='10.227.58.87', username='root', password='0266007766')
t = client.get_transport()
sftp=paramiko.SFTPClient.from_transport(t)

try:
    file_list = ['NLP-Audience.txt','NLP-CN_words.txt','NLP-Event.txt','NLP-Org.txt','NLP-People.txt','NLP-Region.txt','NLP-stopwords.txt']
    for file in file_list:
        print('downloading {}'.format(file))
        sftp.get('/var/www/html/keyword/NLP/{}'.format(file), 'NLP/{}'.format(file))

except Exception as errormsg:
    print('downloading failed...')
    print(errormsg)
    pass

#### ä¸‹è¼‰NLPå­—è©åº«-å®Œæˆ  ####




#### æ“·å–googleé—œéµå­—  ####

time_trend = datetime.now().strftime('%Y%m%d')
time_trend = str(time_trend)

# å˜—è©¦ç·¨ç¢¼æ“·å–è³‡æ–™ï¼Œè‹¥å‡ºç¾éŒ¯èª¤å°±ä¸­æ–·åŸ·è¡Œ

try:
    # ä¿®æ”¹ç¶²å€ç‚ºç›®å‰æ™‚é–“
    trends_url = 'https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&tz=-480&ed={}&geo=TW&ns=15'.format(time_trend)

    res_trends = requests.get(trends_url)

    # æŠŠå¹²æ“¾æ–‡å­— )]}',\n åˆªé™¤ 
    res_trends = re.sub(r'\)\]\}\',\n', '', res_trends.text)

    # è½‰ç¢¼ï¼Œä¸ç„¶ä¸èƒ½çœ‹

    trends = res_trends.encode('utf-8').decode('unicode_escape')


    # æŠŠjsonæª”å†è½‰æˆå­—ä¸²é–‹å§‹åšæ­£å‰‡è™•ç†
    trends_str = str(trends)
    trends_str = trends_str.replace('\"','\'')

    trends_content = re.findall("'query'.*?'snippet':",trends_str)

    trends_title_list = []
    trends_search_list = []
    trends_articles_list = []
    trends_url_list = []
    count_trends = 0

    for content in trends_content:
        count_trends+=1
        # æ´—å‡ºæ¨™é¡Œ
        trends_title = re.findall("'query'.*?,",content)
        trends_title = trends_title[0]
        trends_title = trends_title[9:-2]
        trends_title_list.append(trends_title)

        # æ´—å‡ºæœå°‹é‡
        trends_search = re.findall("formattedTraffic.*?,",content)
        trends_search = trends_search[0]
        trends_search = trends_search[19:-2]
        trends_search_list.append(trends_search)    

        # æ´—å‡ºç¬¬ä¸€å‰‡æ–‡ç« 
        trends_articles = re.findall("'articles.*?timeAgo",content)
        trends_articles = trends_articles[0]
        trends_articles = trends_articles[22:-10]
        trends_articles_list.append(trends_articles)
        
        # æ´—å‡ºæ–‡ç« è¶…é€£çµï¼ŒæŠ“ä¸åˆ°å°±ç”¨googleæœå°‹æ›¿ä»£
        if 'newsUrl' or '\'Url' in content:
            trends_url = re.findall("newsUrl.*?',|'url':.*?',",content)
            trends_url = trends_url[0]
            trends_url = re.findall("http.*?',",content)
            trends_url = trends_url[0]
            trends_url = trends_url[:-2]
            trends_url_list.append(trends_url)
        else:
            trends_url = 'https://www.google.com/search?q='+trends_title
            trends_url_list.append(trends_url)


    print('google access ok')

    print('ç²å–æ¨™é¡Œç­†æ•¸å…±ï¼š {} ç­†'.format(len(trends_title_list)))
    print('ç²å–æœå°‹é‡ç­†æ•¸å…±ï¼š {} ç­†'.format(len(trends_search_list)))
    print('ç²å–å…§æ–‡ç­†æ•¸å…±ï¼š {} ç­†'.format(len(trends_articles_list)))
    print('ç²å–è¶…é€£çµç­†æ•¸å…±ï¼š {} ç­†'.format(len(trends_url_list)))


    ################ ç·¨å¯«ç¶²é å…§å®¹  ################

    rw = open('separate/google.html','w',encoding = 'utf8')
    # r è®€å–
    # w å¯«å…¥(åˆªé™¤åŸæœ¬å…§å®¹)
    # a è¿½åŠ å¯«å…¥


    # å¯«å…¥google trends
    title_trends = '<h2 style="text-align:center;background: #f00; color: #fff; margin: 0 4px; border-radius: 4px;">Googleæœå°‹è¶¨å‹¢</h2>'
    rw.write(title_trends)

    # å¯«å…¥è³‡æ–™æ“·å–æ™‚é–“
    time_now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    time_news = '<div><h3 style="text-align:center;color:gray">è³‡æ–™æ“·å–æ™‚é–“{}</h2></div>'.format(time_now)
    rw.write(time_news)

    for num in range(10):
        begin = '<item>'
        trends_title = (
            '<div style="text-align:left;font-size:20px;">'+
            '<span style="margin-left:20px;font-family:Lucida Console;color:#008000;">({})</span>'.format(num+1)+
            '<span style="margin-left:50px;margin-bottom:5px;font-weight:bold;">{}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>'.format(trends_title_list[num])+
            '<span>ğŸ”¥&nbsp;{}ç­†æœå°‹</span></div>'.format(trends_search_list[num])
            )
        news = '<div style="text-align:left;margin-bottom:25px;margin-left:50px;font-size:20px;"><a href="{}" target="_blank">{}</a></div>'.format(trends_url_list[num],trends_articles_list[num])
        end = '</item>'
        rw.write(begin+trends_title+news+end)
        
except Exception as errormsg:
    print('googleè³‡æ–™ç²å–å¤±æ•—')
    today = datetime.now().strftime('%Y-%m-%d %Hæ™‚')
    message = 'googleè³‡æ–™ç²å–æœ‰èª¤ï¼Œç™¼ç”Ÿæ™‚é–“é»ï¼š{}\n'.format(today)
    lineNotifyMessage(token, message+str(errormsg))
    time.sleep(5)
    os._exit(0)

rw.close()


### NLPå­—åº«æ›´æ–° ###
try:
    '''æŠŠæ’åå‰100å¤§å¯«é€²è©åº«å…§'''
    document = set([line.strip() for line in open("NLP/NLP-CN_words.txt",encoding="utf-8").readlines()])

    '''æª¢æŸ¥æ–‡å­—æ˜¯å¦åŒ…å«åœ¨ Stop å…§'''
    def wirte_in_doc(path):
        with open(path, 'a',encoding='utf-8') as f:
            for i in trends_title_list:
                if i not in document:
                    f.write("\n"+i)
        f.close()

    wirte_in_doc('NLP/NLP-CN_words.txt')
    wirte_in_doc('NLP/NLP-Event.txt')
except:
    print("Jerry è©åº«åŸ·è¡Œå¤±æ•—")



#### ä¸Šå‚³æ›´æ–°çš„NLPå­—è©åº«  ####

try:
    file_list = ['NLP-Audience.txt','NLP-CN_words.txt','NLP-Event.txt','NLP-Org.txt','NLP-People.txt','NLP-Region.txt','NLP-stopwords.txt']
    for file in file_list:
        print('uploading {}'.format(file))
        sftp.put('NLP/{}'.format(file) , '/var/www/html/keyword/NLP/{}'.format(file))

except Exception as errormsg:
    print('uploading failed...')
    print(errormsg)
    pass

#### ä¸Šå‚³æ›´æ–°çš„NLPå­—è©åº«-å®Œæˆ  ####